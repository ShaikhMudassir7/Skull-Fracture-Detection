{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will try to implement a vgg 16 model pretrained on imagenet provided in keras\n",
    "\n",
    "vgg16 usage:\n",
    "keras.applications.vgg16.VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from keras.applications.vgg16 import decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Images Shape: (224, 10738, 224)\n",
      "Testing Images Shape: (224, 2685, 224)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the directories where your slices and labels are stored\n",
    "slice_dir = 'D:/CQ500/ConvOuch-master/ConvOuch-master/Slices'  # Directory where your image slices are saved as .npy files\n",
    "label_dir = 'D:/CQ500/ConvOuch-master/ConvOuch-master/Labels'  # Directory where your labels are saved as .npy files\n",
    "\n",
    "# Initialize lists to hold the image data and labels\n",
    "train_img = []\n",
    "train_label = []\n",
    "test_img = []\n",
    "test_label = []\n",
    "\n",
    "# Load training data\n",
    "for file_name in os.listdir(slice_dir):\n",
    "    if file_name.endswith('.npy'):\n",
    "        # Construct full path to the image slice\n",
    "        img_path = os.path.join(slice_dir, file_name)\n",
    "        \n",
    "        # Load the image slice\n",
    "        img = np.load(img_path)\n",
    "        \n",
    "        # Determine corresponding label file\n",
    "        label_file_name = file_name.replace('Slice', 'Slice').replace('Slices', 'Labels')\n",
    "        label_path = os.path.join(label_dir, label_file_name)\n",
    "        \n",
    "        # Load the label\n",
    "        label_info = np.load(label_path, allow_pickle=True).item()  # `allow_pickle=True` is necessary because the label is a dictionary\n",
    "        label = label_info['label']\n",
    "        \n",
    "        # Append to the respective training lists\n",
    "        train_img.append(img)\n",
    "        train_label.append(label)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "train_img = np.array(train_img)\n",
    "train_label = np.array(train_label)\n",
    "\n",
    "# Optionally split some data into a test set\n",
    "# (This example assumes you're using the first 80% for training and the rest for testing)\n",
    "split_index = int(0.8 * len(train_img))\n",
    "test_img = train_img[split_index:]\n",
    "test_label = train_label[split_index:]\n",
    "train_img = train_img[:split_index]\n",
    "train_label = train_label[:split_index]\n",
    "\n",
    "# Move the number of images axis to the first one\n",
    "train_img = np.moveaxis(train_img, -1, 0)\n",
    "test_img = np.moveaxis(test_img, -1, 0)\n",
    "\n",
    "print(\"Training Images Shape:\", train_img.shape)\n",
    "print(\"Testing Images Shape:\", test_img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming train_img is a list of images with shape (224, 224) or (224, 224, 1)\n",
    "train_img_resized = []\n",
    "\n",
    "for img in train_img:\n",
    "    # If the image is grayscale (224, 224), convert to (224, 224, 3)\n",
    "    if img.ndim == 2:\n",
    "        img = np.stack((img,) * 3, axis=-1)\n",
    "    # If the image is already (224, 224, 1), squeeze and stack\n",
    "    elif img.shape[-1] == 1:\n",
    "        img = np.squeeze(img, axis=-1)\n",
    "        img = np.stack((img,) * 3, axis=-1)\n",
    "    # Resize to (224, 224, 3)\n",
    "    img_resized = resize(img, (224, 224, 3))\n",
    "    train_img_resized.append(img_resized)\n",
    "\n",
    "train_img_resized = np.array(train_img_resized)\n",
    "\n",
    "# Now predict\n",
    "yhat = vgg16_model_complete.predict(train_img_resized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#we will first try a complete vgg16 model for prediction\n",
    "vgg16_model_complete = VGG16(include_top=True, weights='imagenet', \n",
    "                    input_tensor=None, input_shape=(224, 224, 3), pooling=None)\n",
    "\n",
    "print(vgg16_model_complete.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade'\n",
      " 'window_shade' 'window_shade' 'window_shade' 'window_shade']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# convert the probabilities to class labels\n",
    "label = decode_predictions(yhat)\n",
    "\n",
    "#convert to a numpy array\n",
    "label = np.array(label)\n",
    "\n",
    "label\n",
    "\n",
    "predicted_label = label[:, 0, 1]\n",
    "\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try to remove the top 3 fully connected layers and try to fine tune it for our own classification purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 134,264,641\n",
      "Trainable params: 134,264,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#add our own fully connected layers for the final classification\n",
    "\n",
    "# create the base pre-trained model\n",
    "base_model = VGG16(include_top=False, weights='imagenet', \n",
    "                    input_tensor=None, input_shape=(224, 224, 3), pooling=None)\n",
    "\n",
    "x = base_model.output\n",
    "#flattentit\n",
    "x = Flatten()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(4096, activation='relu')(x)\n",
    "#another fully-connected layer\n",
    "x = Dense(4096, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(1, kernel_initializer='normal', activation='sigmoid')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we can start to fine-tune the model\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you've already trained your VGG16 model\n",
    "\n",
    "# Save the model\n",
    "model.save('vgg16_model.h5')\n",
    "\n",
    "# Later, when you want to use the model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model\n",
    "model = load_model('vgg16_model.h5')\n",
    "\n",
    "# # Make predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 33-34: truncated \\UXXXXXXXX escape (<ipython-input-21-6eee08aca6c4>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-21-6eee08aca6c4>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    img_path = 'D:\\CQ500\\CQ500 dataset\\CQ500-CT-0\\Unknown Study\\CT 4cc sec 150cc D3D on\\CT000000.dcm'\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 33-34: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "model = load_model('vgg16_model.h5')\n",
    "\n",
    "# Path to the image file\n",
    "img_path = 'D:\\CQ500\\CQ500 dataset\\CQ500-CT-0\\Unknown Study\\CT 4cc sec 150cc D3D on\\CT000000.dcm'\n",
    "predictions = model.predict(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
